{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 2\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 2 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/).\n",
    "\n",
    "**Se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar dependencias\n",
    "Para esta tarea, se han agregado algunos requerimientos, asegúrese de instalarlos (puede usar el mismo entorno virtual de la Tarea 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión a la Base y Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data\") / \"shakespeare\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_table(table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "print(\"Conectando a la base...\")\n",
    "conn_str = \"mysql+pymysql://guest:relational@relational.fit.cvut.cz:3306/Shakespeare\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Todos los párrafos de todas las obras\n",
    "df_paragraphs = load_table(\"paragraphs\", engine)\n",
    "\n",
    "df_characters = load_table(\"characters\", engine)\n",
    "\n",
    "df_works = load_table(\"works\", engine)\n",
    "\n",
    "df_chapters = load_table(\"chapters\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Actualizar con su versión de clean_text() de la Tarea_1\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    # Convertir todo a minúsculas\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    # Quitar signos de puntuación y cambiarlos por espacios (\" \")\n",
    "    # Se incluyen los signos de puntuación buscados antes (excepto el apóstrofe)\n",
    "    signos = [\"[\", \"\\n\", \",\", \"]\", \".\", \";\", \"?\", \"!\", \":\", \"-\", \"(\", \")\", \"&\",'\"', \"\\t\"]\n",
    "    for punc in signos:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result\n",
    "\n",
    "#def expand_contractions(text):\n",
    "#    out = con.fix(text, slang=False)\n",
    "#    return out\n",
    "\n",
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "# Se eliminan las contracciones mediante el uso de contractions.\n",
    "#df_paragraphs['CleanContractions'] = df_paragraphs['CleanText'] #.apply(expand_contractions)\n",
    "#df_paragraphs['CleanContractions']= df_paragraphs['CleanContractions'].str.lower()\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos personajes, obras y géneros en el mismo dataset\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Antony\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Features de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Partir train/test 30% estratificados\n",
    "# -> Definir X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeo tamaño de test del 30%\n",
    "print(f\"Tamaño DataSet: {len(X)}\")\n",
    "print(f\"Tamaños de Train/Test: {len(X_train)}/{len(X_test)}\")\n",
    "print(f\"Porcentaje Test: {'{0:.2f}'.format(len(X_test)*100/len(X))} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()\n",
    "totalSize = len(X)\n",
    "totalCharacters = dict(Counter(df_dataset[\"CharName\"]))\n",
    "\n",
    "for key, value in totalCharacters.items():\n",
    "    print(f\"{key} : {value}\")\n",
    "    p = '{0:.2f}'.format(int(value)*100/totalSize)\n",
    "    print(f\"Porcentaje Character {key}:  {p} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeo muestreoestratificado (propociones de personajes se mantengan en conjunto de entrenamiento)\n",
    "cols = ['CharName', 'Percent', 'Type']\n",
    "df_train = pd.DataFrame(columns = cols) \n",
    "me_y_train = dict(Counter(y_train))\n",
    "for key, value in me_y_train.items():\n",
    "    print(f\"{key} : {value}\")\n",
    "    p = '{0:.2f}'.format(int(value)*100/(len(X_train)))\n",
    "    print(f\"Porcentaje en muestra Character {key}:  {p} %\")\n",
    "    df_train.loc[len(df_train)] = {'CharName': key, 'Percent': p, 'Type' : 'Train'}\n",
    "    #df_train = pd.concat([df_train, {'CharName': key, 'Percent': p, 'Type' : 'Train'}],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeo muestreoestratificado (propociones de personajes se mantengan en conjunto de test)\n",
    "df_test = pd.DataFrame(columns = cols)\n",
    "me_y_test = dict(Counter(y_test))\n",
    "for key, value in me_y_test.items():\n",
    "    print(f\"{key} : {value}\")\n",
    "    p = '{0:.2f}'.format(int(value)*100/(len(X_test)))\n",
    "    print(f\"Porcentaje en test Character {key}:  {p} %\")\n",
    "    df_test.loc[len(df_test)] = {'CharName': key, 'Percent': p, 'Type' : 'Test'}\n",
    "    #df_test = df_test.append({'CharName': key, 'Percent': p, 'Type' : 'Test'},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Data\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('pastel')\n",
    "df_train = df_train.sort_values('CharName', ascending=True)\n",
    "#sns.catplot(data=final_df, x='CharName', y='Percent', hue='Type')\n",
    "plt.pie(df_train['Percent'], labels=df_train['CharName'], autopct = '%0.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.sort_values('CharName', ascending=True)\n",
    "# Visualize the Data\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('pastel')\n",
    "#sns.catplot(data=final_df, x='CharName', y='Percent', hue='Type')\n",
    "plt.pie(df_test['Percent'], labels=df_test['CharName'], autopct = '%0.0f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the Data\n",
    "\n",
    "df_alltrain = pd.DataFrame()\n",
    "df_alltrain['CharName'] = y_train\n",
    "df_alltrain['CleanText'] = X_train\n",
    "df_alltrain['Type'] = 'Train'\n",
    "\n",
    "df_alltest = pd.DataFrame()\n",
    "df_alltest['CharName'] = y_test\n",
    "df_alltest['CleanText'] = X_test\n",
    "df_alltest['Type'] = 'Test'\n",
    "\n",
    "final_df = pd.concat([df_alltrain,df_alltest], ignore_index=True)\n",
    "#final_df = df_train.append(df_test, ignore_index=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Data\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('pastel')\n",
    "#sns.catplot(data=final_df, x='CharName', y='Percent', hue='Type')\n",
    "sns.histplot(data=final_df,  x='CharName', hue='Type', binwidth=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(np.array([X_train, y_train]).T, columns=['Paragraphs', 'CharName'])\n",
    "df_train['Tipo']='Train'\n",
    "df_test = pd.DataFrame(np.array([X_test, y_test]).T, columns=['Paragraphs', 'CharName'])\n",
    "df_test['Tipo'] = 'Test'\n",
    "df_tot = pd.DataFrame(np.array([X, y]).T, columns=['Paragraphs', 'CharName'])\n",
    "df_tot['Tipo'] = 'Total'\n",
    "df_TT = pd.concat([df_train, df_test, df_tot], ignore_index=True)\n",
    "\n",
    "ax = sns.histplot(x=df_TT[\"CharName\"],  hue=df_TT[\"Tipo\"],  multiple=\"dodge\", shrink=0.9, stat='density', common_norm=False)\n",
    "ax.set_xlabel('Nombre del personaje')\n",
    "ax.set_ylabel('Porcentaje de párrafos asignado')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = count_vect.get_feature_names_out()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,1))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term-frequency times inverse document-frequency\n",
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Realizar PCA sobre los datos de entrenamiento\n",
    "reductor = PCA(n_components = 5)\n",
    "\n",
    "# Transformar train\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las dos primeras componentes de PCA\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "\n",
    "ax.set_title(\"PCA por personaje\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductor.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación:\n",
    "VE = []\n",
    "\n",
    "# Sin nada:\n",
    "reductor = PCA(n_components = 10)\n",
    "# Transformar train\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n",
    "VE_control = reductor.explained_variance_\n",
    "VE.append(VE_control)\n",
    "\n",
    "# Sin stop words\n",
    "\n",
    "additional_stop_words=['thou', 'thee', 'thy', 'ye', 'thine', ]\n",
    "count_vect = CountVectorizer(stop_words=list(text.ENGLISH_STOP_WORDS.union(additional_stop_words)), ngram_range=(1,1))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "reductor = PCA(n_components = 10)\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n",
    "VE_SW = reductor.explained_variance_\n",
    "VE.append(VE_SW)\n",
    "\n",
    "# Usando IDF\n",
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,1))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tf_idf = TfidfTransformer(use_idf=True)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "reductor = PCA(n_components = 10)\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n",
    "VE_IDF = reductor.explained_variance_\n",
    "VE.append(VE_IDF)\n",
    "\n",
    "# Con bigrama\n",
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,2))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "reductor = PCA(n_components = 10)\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())\n",
    "VE_BG = reductor.explained_variance_\n",
    "VE.append(VE_BG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA = pd.DataFrame(VE_control, columns=['VarExplicada'])\n",
    "df_PCA['Tipo']= 'Simple'\n",
    "\n",
    "auxSW = pd.DataFrame(VE_SW, columns=['VarExplicada'])\n",
    "auxSW['Tipo']= 'Sin Stopwords'\n",
    "\n",
    "auxIDF = pd.DataFrame(VE_IDF, columns=['VarExplicada'])\n",
    "auxIDF['Tipo']= 'TF-IDF'\n",
    "\n",
    "auxBG = pd.DataFrame(VE_SW, columns=['VarExplicada'])\n",
    "auxBG['Tipo']= 'Bigrama'\n",
    "\n",
    "df_PCA = pd.concat([df_PCA, auxSW, auxIDF, auxBG], ignore_index=False).reset_index()\n",
    "df_PCA['index'] = df_PCA['index']+1\n",
    "df_PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=df_PCA[\"index\"], y=df_PCA[\"VarExplicada\"], hue=df_PCA[\"Tipo\"])\n",
    "ax.set_xlabel('Componentes del PCA')\n",
    "ax.set_ylabel('Varianza explicada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#le = preprocessing.LabelEncoder()\n",
    "#y_train_enc = le.fit_transform(y_train)\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,2))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf.fit_transform(X_train_counts)\n",
    "\n",
    "bayes_clf = MultinomialNB().fit(X_train_tf, y_train)\n",
    "\n",
    "# Ver las primeras 10 predicciones de train\n",
    "y_pred_train = bayes_clf.predict(X_train_tf)\n",
    "y_pred_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Predecir para test y ver la matriz de confusión, y reportar accuracy\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "X_test_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tf.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = bayes_clf.predict(X_test_tf)\n",
    "get_accuracy(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred, label):\n",
    "    yt = (y_true == label)\n",
    "    yp = (y_pred == label)\n",
    "    return (yt*yp).sum()/yp.sum()\n",
    "\n",
    "def recall(y_true, y_pred, label):\n",
    "    yt = (y_true == label)\n",
    "    yp = (y_pred == label)\n",
    "    return (yt*yp).sum()/yt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision y recall\n",
    "print('\\t \\t Antony | Queen Margaret | Cleopatra')\n",
    "print('Precision \\t  %.2f \\t       %.2f  \\t     %.2f '%(precision(y_test, y_test_pred,'Antony'), precision(y_test, y_test_pred,'Queen Margaret'), precision(y_test, y_test_pred,'Cleopatra')))\n",
    "print('Recall \\t \\t  %.2f \\t       %.2f  \\t     %.2f '%(recall(y_test, y_test_pred,'Antony'), recall(y_test, y_test_pred,'Queen Margaret'), recall(y_test, y_test_pred,'Cleopatra')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de hiper-parámetros con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TODO: Agregar más variantes de parámetros que les parezcan relevantes\n",
    "param_sets = [{\"stop_words\": None, \"ngram\": (1,2), \"idf\": True},\n",
    "             {\"stop_words\": None, \"ngram\": (1,1), \"idf\": False}]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Ahora usaremos train/validation/test\n",
    "# Por lo tanto le renombramos train+validation = dev(elopment) dataset\n",
    "X_dev = X_train\n",
    "y_dev = y_train\n",
    "acc_dev = []\n",
    "\n",
    "# # Para evitar errores\n",
    "# del X_train\n",
    "# del y_train\n",
    "\n",
    "for params in param_sets:\n",
    "    \n",
    "    # Transormaciones a aplicar (featurizers)\n",
    "    count_vect = CountVectorizer(stop_words=params[\"stop_words\"], ngram_range=params[\"ngram\"])\n",
    "    tf_idf = TfidfTransformer(use_idf=params[\"idf\"])\n",
    "    \n",
    "    for train_idxs, val_idxs in skf.split(X_dev, y_dev):\n",
    "        \n",
    "        # Train y validation para el split actual\n",
    "        X_train_ = X_dev[train_idxs]\n",
    "        y_train_ = y_dev[train_idxs]\n",
    "        X_val = X_dev[val_idxs]\n",
    "        y_val = y_dev[val_idxs]\n",
    "        \n",
    "        # Ajustamos y transformamos Train\n",
    "        X_train_counts = count_vect.fit_transform(X_train_)\n",
    "        X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "        \n",
    "        # TODO: Completar el código para entrenar y evaluar \n",
    "        \n",
    "        # Entrenamos con Train\n",
    "        bayes_clf = MultinomialNB().fit(X_train_tf, y_train_)\n",
    "\n",
    "        # Transformamos Validation\n",
    "        X_val_counts = count_vect.transform(X_val)\n",
    "        X_val_tfidf = tf_idf.transform(X_val_counts)\n",
    "        \n",
    "        # Predecimos y evaluamos en Validation\n",
    "        y_pred_val = bayes_clf.predict(X_val_tfidf)\n",
    "        acc = get_accuracy(y_val, y_pred_val)\n",
    "        acc_dev.append(acc)\n",
    "        #print(f\"{acc=:.4f} {params=}\")\n",
    "        print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia \n",
    "X_train_knn = X_train\n",
    "y_train_knn = y_train\n",
    "X_test_knn = X_test\n",
    "y_test_knn = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar\n",
    "# Transormaciones a aplicar (featurizers)\n",
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,2))\n",
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "\n",
    "# Ajustamos y transformamos Train\n",
    "X_train_knn_counts = count_vect.fit_transform(X_train_knn)\n",
    "X_train_knn_tf = tf_idf.fit_transform(X_train_knn_counts)\n",
    "\n",
    "\n",
    "X_test_knn_counts = count_vect.transform(X_test_knn)\n",
    "X_test_knn_tfidf = tf_idf.transform(X_test_knn_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_knn_tf, y_train_knn)\n",
    "score = knn.score(X_train_knn_tf, y_train_knn)\n",
    "print(\"Training score: \", score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir\n",
    "y_pred_knn = knn.predict(X_test_knn_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar\n",
    "acc_knn = get_accuracy(y_test_knn, y_pred_knn)\n",
    "print(\"Acc knn:\", acc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_knn, y_pred_knn)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(y_test_knn, y_pred_knn)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  NCA y KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_knn_nca = X_train\n",
    "y_train_knn_nca = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca = NeighborhoodComponentsAnalysis(random_state=1)\n",
    "knn_nca = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_knn_nca_counts = count_vect.fit_transform(X_train_knn_nca)\n",
    "X_train_knn_nca_tf = tf_idf.fit_transform(X_train_knn_nca_counts)\n",
    "\n",
    "X_test_knn_nca_counts = count_vect.transform(X_test_knn)\n",
    "X_test_knn_nca_tfidf = tf_idf.transform(X_test_knn_counts)\n",
    "\n",
    "#nca.fit(X_train_knn_nca_tf, y_train_knn_nca)\n",
    "#knn_nca.fit(nca.transform(X_train_knn_nca_tf), y_train_knn_nca)\n",
    "# Compute the nearest neighbor accuracy on the embedded test set\n",
    "#acc_knn_nca = knn_nca.score(nca.transform(X_test_knn_tfidf), y_test_knn)\n",
    "#acc_knn_nca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Opcional) Comparativa con Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "y_train_s = np.char.replace(y_train.astype(str), \" \", \"_\").astype(object)\n",
    "y_test_s = np.char.replace(y_test.astype(str), \" \", \"_\").astype(object)\n",
    "\n",
    "# Convertimos al formato de fasttext: archivo de texto donde cada línea es:\n",
    "# __label__<label> TEXTO\n",
    "Xytrains = \"__label__\" + y_train_s.astype(object) + \" \" + X_train\n",
    "Xytests = \"__label__\" + y_test_s.astype(object) + \" \" + X_test\n",
    "np.savetxt(data_dir / \"train.txt\", Xytrains, fmt=\"%s\")\n",
    "np.savetxt(data_dir / \"test.txt\", Xytests, fmt=\"%s\")\n",
    "\n",
    "Xytests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=str(data_dir / \"train.txt\"), epoch=100, wordNgrams=2)\n",
    "model.test(str(data_dir / \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = model.predict(list(X_test))\n",
    "y_pred_test = [y[0].replace(\"__label__\", \"\") for y in y_out[0]]\n",
    "    \n",
    "print(get_accuracy(y_test_s, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
